# PIPELINE DEFINITION
# Name: penguins-pipeline
# Inputs:
#    data_bucket: str
#    data_file: str
#    model_dir: str
#    project_id: str
#    run_id: str [Default: 'manual-run']
components:
  comp-download-data:
    executorLabel: exec-download-data
    inputDefinitions:
      parameters:
        bucket:
          parameterType: STRING
        file_name:
          parameterType: STRING
        project_id:
          parameterType: STRING
    outputDefinitions:
      artifacts:
        dataset:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
  comp-evaluate-and-promote:
    executorLabel: exec-evaluate-and-promote
    inputDefinitions:
      artifacts:
        model_art:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
        test_ds:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
      parameters:
        delta:
          defaultValue: 0.0
          isOptional: true
          parameterType: NUMBER_DOUBLE
        model_dir:
          parameterType: STRING
        promote_mode:
          defaultValue: if_better
          isOptional: true
          parameterType: STRING
        run_id:
          defaultValue: ''
          isOptional: true
          parameterType: STRING
    outputDefinitions:
      artifacts:
        metrics:
          artifactType:
            schemaTitle: system.Metrics
            schemaVersion: 0.0.1
  comp-preprocess:
    executorLabel: exec-preprocess
    inputDefinitions:
      artifacts:
        dataset:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
    outputDefinitions:
      artifacts:
        test_out:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        train_out:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
  comp-train:
    executorLabel: exec-train
    inputDefinitions:
      artifacts:
        train_ds:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
    outputDefinitions:
      artifacts:
        model_art:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
deploymentSpec:
  executors:
    exec-download-data:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - download_data
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'pandas' 'google-cloud-storage'\
          \ 'packaging'  &&  python3 -m pip install --quiet --no-warn-script-location\
          \ 'kfp==2.14.6' '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"\
          3.9\"' && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef download_data(project_id: str,\n                  bucket: str,\n\
          \                  file_name: str,\n                  dataset: Output[Dataset]):\n\
          \    \"\"\"Download data\"\"\"\n    from google.cloud import storage\n \
          \   import pandas as pd\n    import logging\n    import sys\n\n    logging.basicConfig(stream=sys.stdout,\
          \ level=logging.INFO)\n\n    client = storage.Client(project=project_id)\n\
          \    bucket = client.bucket(bucket)\n    blob = bucket.blob(file_name)\n\
          \    local_csv = dataset.path + '.csv'\n    blob.download_to_filename(local_csv)\n\
          \    logging.info(f\"Downloaded to {local_csv}\")\n\n"
        image: python:3.10.7-slim
    exec-evaluate-and-promote:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - evaluate_and_promote
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'pandas' 'joblib'\
          \ 'fsspec' 'gcsfs' 'scikit-learn' 'google-cloud-storage' 'packaging'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'kfp==2.14.6'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"\
          $0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef evaluate_and_promote(\n    test_ds: Input[Dataset],         \
          \ # expects test_ds.path/test.csv\n    model_art: Input[Model],        \
          \  # expects model_art.path/{model.pkl, model_meta.json}\n    metrics: Output[Metrics],\n\
          \    model_dir: str,                   # champion pad, e.g. gs://assignment1group3/models\n\
          \    run_id: str = \"\",                 # voor archiveren\n    promote_mode:\
          \ str = \"if_better\",  # \"always\" of \"if_better\"\n    delta: float\
          \ = 0.0                # minimaal vereiste verbetering t.o.v. champion\n\
          ):\n    \"\"\"\n    Evaluates the model and promotes if accuracy of model\
          \ > accuracy previous champion model\n    \"\"\"\n    import os, sys, json,\
          \ logging\n    import pandas as pd\n    import joblib\n    from sklearn.metrics\
          \ import accuracy_score\n    from sklearn.preprocessing import LabelEncoder\n\
          \    import fsspec\n    from google.cloud import storage\n\n    logging.basicConfig(stream=sys.stdout,\
          \ level=logging.INFO)\n\n    # ==== Locaties ====\n    test_csv  = os.path.join(test_ds.path,\
          \ \"test.csv\")\n    model_pkl = os.path.join(model_art.path, \"model.pkl\"\
          )\n    meta_json = os.path.join(model_art.path, \"model_meta.json\")\n \
          \   champion_dir = model_dir.rstrip(\"/\")\n\n    # ==== Data & model laden\
          \ ====\n    logging.info(f\"[eval] reading test={test_csv}\")\n    df =\
          \ pd.read_csv(test_csv)\n    X = df[[\"bill_length_mm\",\"bill_depth_mm\"\
          ,\"flipper_length_mm\",\"body_mass_g\"]]\n    y = df[\"species\"]\n\n  \
          \  logging.info(f\"[eval] loading model={model_pkl}\")\n    model = joblib.load(model_pkl)\n\
          \n    with open(meta_json, \"r\") as f:\n        meta = json.load(f)\n \
          \   classes = meta[\"classes\"]\n\n    le = LabelEncoder()\n    le.fit(classes)\n\
          \    y_encoded = le.transform(y)\n\n    # Evaluate\n    y_pred = model.predict(X)\n\
          \    acc = float(accuracy_score(y_encoded, y_pred))\n    metrics.log_metric(\"\
          accuracy\", acc)\n    logging.info(f\"[eval] accuracy={acc:.6f}\")\n\n \
          \   # Get accuracy from previous champion\n    prev_acc = 0.0\n    try:\n\
          \        if champion_dir.startswith(\"gs://\"):\n            # gebruik GCS\
          \ client hier voor betrouwbare exists()\n            _, rest = champion_dir[5:].split(\"\
          /\", 1) if \"/\" in champion_dir[5:] else (champion_dir[5:], \"\")\n   \
          \         bucket_name, prefix = champion_dir[5:].split(\"/\", 1)\n     \
          \       metrics_blob = f\"{prefix.rstrip('/')}/metrics.json\"\n        \
          \    gcs = storage.Client()\n            bkt = gcs.bucket(bucket_name)\n\
          \            bl = bkt.blob(metrics_blob)\n            if bl.exists():\n\
          \                prev = json.loads(bl.download_as_text())\n            \
          \    prev_acc = float(prev.get(\"accuracy\", 0.0))\n    except Exception\
          \ as e:\n        logging.warning(f\"[eval] could not read previous metrics:\
          \ {e}\")\n\n    logging.info(f\"[eval] prev_accuracy={prev_acc:.6f}, delta={delta:.6f}\"\
          )\n\n    # ==== Bepalen of we promoten ====\n    do_promote = (promote_mode\
          \ == \"always\") or (acc >= prev_acc + delta)\n    logging.info(f\"[eval]\
          \ promote_mode={promote_mode}, do_promote={do_promote}\")\n\n    # ====\
          \ Run-metrics altijd wegschrijven (archief) ====\n    # Run-artefacten (goed\
          \ voor debugging / lineage)\n    fs = fsspec.filesystem(\"gcs\")\n    if\
          \ run_id:\n        run_root = f\"{champion_dir}/archive/run_id={run_id}\"\
          \n        fs.makedirs(run_root, exist_ok=True)\n        fs.put(model_pkl,\
          \ f\"{run_root}/model.pkl\")\n        fs.put(meta_json, f\"{run_root}/model_meta.json\"\
          )\n        with fs.open(f\"{run_root}/metrics.json\", \"w\") as f:\n   \
          \         json.dump({\"accuracy\": acc}, f)\n\n    # Change the champion\
          \ model (if we want to promote)\n    if do_promote:\n        fs.put(model_pkl,\
          \ f\"{champion_dir}/model.pkl\")\n        fs.put(meta_json, f\"{champion_dir}/model_meta.json\"\
          )\n        with fs.open(f\"{champion_dir}/metrics.json\", \"w\") as f:\n\
          \            json.dump({\"accuracy\": acc}, f)\n        logging.info(f\"\
          [eval] promoted model to {champion_dir}\")\n    else:\n        logging.info(\"\
          [eval] not promoted (gate blocked)\")\n\n    # Decision-log\n    decision\
          \ = {\n        \"promoted\": do_promote,\n        \"promote_mode\": promote_mode,\n\
          \        \"accuracy_new\": acc,\n        \"accuracy_prev\": prev_acc,\n\
          \        \"delta\": delta,\n        \"run_id\": run_id,\n    }\n    if run_id:\n\
          \        with fs.open(f\"{champion_dir}/archive/run_id={run_id}/decision.json\"\
          , \"w\") as f:\n            json.dump(decision, f)\n\n"
        image: python:3.10-slim
    exec-preprocess:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - preprocess
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'pandas' 'scikit-learn'\
          \  &&  python3 -m pip install --quiet --no-warn-script-location 'kfp==2.14.6'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"\
          $0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef preprocess(dataset: Input[Dataset],\n               train_out:\
          \ Output[Dataset],\n               test_out: Output[Dataset]):\n\n    import\
          \ pandas as pd\n    from sklearn.model_selection import train_test_split\n\
          \    import logging\n    import sys\n    import os\n\n    logging.basicConfig(stream=sys.stdout,\
          \ level=logging.INFO)\n\n    cols = [\"bill_length_mm\",\"bill_depth_mm\"\
          ,\"flipper_length_mm\",\"body_mass_g\",\"species\"]\n    df = pd.read_csv(dataset.path\
          \ + \".csv\")[cols].dropna()\n    train, test = train_test_split(df, test_size=0.2,\
          \ random_state=42, stratify=df[\"species\"])\n\n    os.makedirs(train_out.path,\
          \ exist_ok=True)\n    train_csv_path = os.path.join(train_out.path, \"train.csv\"\
          )\n    train.to_csv(train_csv_path, index=False)\n\n    os.makedirs(test_out.path,\
          \ exist_ok=True)\n    test_csv_path = os.path.join(test_out.path, \"test.csv\"\
          )\n    test.to_csv(test_csv_path, index=False)\n\n"
        image: python:3.10.7-slim
    exec-train:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - train
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'pandas' 'scikit-learn'\
          \ 'joblib'  &&  python3 -m pip install --quiet --no-warn-script-location\
          \ 'kfp==2.14.6' '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"\
          3.9\"' && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef train(\n    train_ds: Input[Dataset],   # expects train_ds.path/train.csv\n\
          \    model_art: Output[Model],   # directory to store model files\n):\n\
          \    \"\"\"Train sklearn pipeline, save model.pkl + model_meta.json into\
          \ model_art.path\"\"\"\n    import os, sys, logging, json, joblib\n    import\
          \ pandas as pd\n    from sklearn.preprocessing import StandardScaler\n \
          \   from sklearn.linear_model import LogisticRegression\n    from sklearn.pipeline\
          \ import Pipeline\n    from sklearn.preprocessing import LabelEncoder\n\n\
          \    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n\n    train_csv\
          \ = os.path.join(train_ds.path, \"train.csv\")\n    logging.info(f\"[train]\
          \ reading {train_csv}\")\n\n    df = pd.read_csv(train_csv)\n    X = df[[\"\
          bill_length_mm\",\"bill_depth_mm\",\"flipper_length_mm\",\"body_mass_g\"\
          ]]\n    y = df[\"species\"]\n\n    le = LabelEncoder()\n    y_encoded =\
          \ le.fit_transform(y)\n\n    pipe = Pipeline(\n        steps=[\n       \
          \     (\"scaler\", StandardScaler()),\n            (\"clf\", LogisticRegression(max_iter=500,\
          \ multi_class=\"ovr\", random_state=42)),\n        ]\n    ).fit(X, y_encoded)\n\
          \n    os.makedirs(model_art.path, exist_ok=True)\n    model_pkl = os.path.join(model_art.path,\
          \ \"model.pkl\")\n    meta_json = os.path.join(model_art.path, \"model_meta.json\"\
          )\n\n    joblib.dump(pipe, model_pkl)\n\n    with open(meta_json, \"w\"\
          ) as f:\n        json.dump({\"classes\": sorted(y.unique())}, f)\n\n   \
          \ logging.info(f\"[train] saved model to {model_pkl}\")\n\n"
        image: python:3.10-slim
pipelineInfo:
  name: penguins-pipeline
root:
  dag:
    tasks:
      download-data:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-download-data
        inputs:
          parameters:
            bucket:
              componentInputParameter: data_bucket
            file_name:
              componentInputParameter: data_file
            project_id:
              componentInputParameter: project_id
        taskInfo:
          name: download-data
      evaluate-and-promote:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-evaluate-and-promote
        dependentTasks:
        - preprocess
        - train
        inputs:
          artifacts:
            model_art:
              taskOutputArtifact:
                outputArtifactKey: model_art
                producerTask: train
            test_ds:
              taskOutputArtifact:
                outputArtifactKey: test_out
                producerTask: preprocess
          parameters:
            model_dir:
              componentInputParameter: model_dir
        taskInfo:
          name: evaluate-and-promote
      preprocess:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-preprocess
        dependentTasks:
        - download-data
        inputs:
          artifacts:
            dataset:
              taskOutputArtifact:
                outputArtifactKey: dataset
                producerTask: download-data
        taskInfo:
          name: preprocess
      train:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-train
        dependentTasks:
        - preprocess
        inputs:
          artifacts:
            train_ds:
              taskOutputArtifact:
                outputArtifactKey: train_out
                producerTask: preprocess
        taskInfo:
          name: train
  inputDefinitions:
    parameters:
      data_bucket:
        parameterType: STRING
      data_file:
        parameterType: STRING
      model_dir:
        parameterType: STRING
      project_id:
        parameterType: STRING
      run_id:
        defaultValue: manual-run
        isOptional: true
        parameterType: STRING
schemaVersion: 2.1.0
sdkVersion: kfp-2.14.6
