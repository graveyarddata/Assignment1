{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "055f581d",
   "metadata": {},
   "source": [
    "Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5534c465",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kfp>=2 in /home/jupyter/.local/lib/python3.10/site-packages (2.14.6)\n",
      "Requirement already satisfied: google-cloud-pipeline-components>=2 in /home/jupyter/.local/lib/python3.10/site-packages (2.21.0)\n",
      "Requirement already satisfied: google-cloud-aiplatform in /home/jupyter/.local/lib/python3.10/site-packages (1.122.0)\n",
      "Requirement already satisfied: click==8.1.8 in /opt/conda/lib/python3.10/site-packages (from kfp>=2) (8.1.8)\n",
      "Requirement already satisfied: click-option-group==0.5.7 in /opt/conda/lib/python3.10/site-packages (from kfp>=2) (0.5.7)\n",
      "Requirement already satisfied: docstring-parser<1,>=0.7.3 in /opt/conda/lib/python3.10/site-packages (from kfp>=2) (0.17.0)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /opt/conda/lib/python3.10/site-packages (from kfp>=2) (2.26.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.1 in /opt/conda/lib/python3.10/site-packages (from kfp>=2) (2.41.1)\n",
      "Requirement already satisfied: google-cloud-storage<4,>=2.2.1 in /opt/conda/lib/python3.10/site-packages (from kfp>=2) (2.19.0)\n",
      "Requirement already satisfied: kfp-pipeline-spec<3,>=2.14.3 in /home/jupyter/.local/lib/python3.10/site-packages (from kfp>=2) (2.14.6)\n",
      "Requirement already satisfied: kfp-server-api<3,>=2.14.3 in /opt/conda/lib/python3.10/site-packages (from kfp>=2) (2.14.3)\n",
      "Requirement already satisfied: kubernetes<31,>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from kfp>=2) (30.1.0)\n",
      "Requirement already satisfied: protobuf<7.0,>=6.31.1 in /opt/conda/lib/python3.10/site-packages (from kfp>=2) (6.31.1)\n",
      "Requirement already satisfied: PyYAML<7,>=5.3 in /opt/conda/lib/python3.10/site-packages (from kfp>=2) (6.0.3)\n",
      "Requirement already satisfied: requests-toolbelt<2,>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from kfp>=2) (1.0.0)\n",
      "Requirement already satisfied: tabulate<1,>=0.8.6 in /opt/conda/lib/python3.10/site-packages (from kfp>=2) (0.9.0)\n",
      "Requirement already satisfied: urllib3<3.0.0 in /opt/conda/lib/python3.10/site-packages (from kfp>=2) (2.5.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp>=2) (1.70.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /opt/conda/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp>=2) (1.26.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /opt/conda/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp>=2) (2.32.5)\n",
      "Requirement already satisfied: cachetools<7.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.1->kfp>=2) (6.2.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.1->kfp>=2) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.1->kfp>=2) (4.9.1)\n",
      "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage<4,>=2.2.1->kfp>=2) (2.4.3)\n",
      "Requirement already satisfied: google-resumable-media>=2.7.2 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage<4,>=2.2.1->kfp>=2) (2.7.2)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage<4,>=2.2.1->kfp>=2) (1.7.1)\n",
      "Requirement already satisfied: six>=1.10 in /opt/conda/lib/python3.10/site-packages (from kfp-server-api<3,>=2.14.3->kfp>=2) (1.17.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from kfp-server-api<3,>=2.14.3->kfp>=2) (2025.10.5)\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.10/site-packages (from kfp-server-api<3,>=2.14.3->kfp>=2) (2.9.0.post0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/conda/lib/python3.10/site-packages (from kubernetes<31,>=8.0.0->kfp>=2) (1.9.0)\n",
      "Requirement already satisfied: requests-oauthlib in /opt/conda/lib/python3.10/site-packages (from kubernetes<31,>=8.0.0->kfp>=2) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in /opt/conda/lib/python3.10/site-packages (from kubernetes<31,>=8.0.0->kfp>=2) (3.3.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp>=2) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp>=2) (3.10)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /opt/conda/lib/python3.10/site-packages (from rsa<5,>=3.1.4->google-auth<3,>=1.6.1->kfp>=2) (0.6.1)\n",
      "Requirement already satisfied: Jinja2<4,>=3.1.2 in /opt/conda/lib/python3.10/site-packages (from google-cloud-pipeline-components>=2) (3.1.6)\n",
      "Requirement already satisfied: packaging>=14.3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (25.0)\n",
      "Requirement already satisfied: google-cloud-bigquery!=3.20.0,<4.0.0,>=1.15.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (3.38.0)\n",
      "Requirement already satisfied: google-cloud-resource-manager<3.0.0,>=1.3.3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (1.14.2)\n",
      "Requirement already satisfied: shapely<3.0.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (2.1.2)\n",
      "Requirement already satisfied: google-genai<2.0.0,>=1.37.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (1.42.0)\n",
      "Requirement already satisfied: pydantic<3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (2.12.0)\n",
      "Requirement already satisfied: typing_extensions in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (4.15.0)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform) (1.75.1)\n",
      "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform) (1.75.1)\n",
      "Requirement already satisfied: grpc-google-iam-v1<1.0.0,>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-resource-manager<3.0.0,>=1.3.3->google-cloud-aiplatform) (0.14.2)\n",
      "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform) (4.11.0)\n",
      "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in /opt/conda/lib/python3.10/site-packages (from google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform) (0.28.1)\n",
      "Requirement already satisfied: tenacity<9.2.0,>=8.2.3 in /opt/conda/lib/python3.10/site-packages (from google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform) (9.1.2)\n",
      "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform) (15.0.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio<5.0.0,>=4.8.0->google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.10/site-packages (from anyio<5.0.0,>=4.8.0->google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform) (1.3.1)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from Jinja2<4,>=3.1.2->google-cloud-pipeline-components>=2) (3.0.3)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3->google-cloud-aiplatform) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.1 in /opt/conda/lib/python3.10/site-packages (from pydantic<3->google-cloud-aiplatform) (2.41.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /opt/conda/lib/python3.10/site-packages (from pydantic<3->google-cloud-aiplatform) (0.4.2)\n",
      "Requirement already satisfied: numpy>=1.21 in /opt/conda/lib/python3.10/site-packages (from shapely<3.0.0->google-cloud-aiplatform) (1.26.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U \"kfp>=2\" \"google-cloud-pipeline-components>=2\" google-cloud-aiplatform\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef586b2",
   "metadata": {},
   "source": [
    "Restart the kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "396ffa92",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib, pkgutil\n",
    "import IPython\n",
    "\n",
    "IPython.get_ipython().kernel.do_shutdown(restart=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "171dbf4d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFP SDK version: 2.14.6\n",
      "google-cloud-pipeline-components version: 2.21.0\n"
     ]
    }
   ],
   "source": [
    "# Imports from the later cell (assuming you run this after the import cell)\n",
    "import kfp\n",
    "import google_cloud_pipeline_components\n",
    "\n",
    "print(f\"KFP SDK version: {kfp.__version__}\")\n",
    "# Note: google-cloud-aiplatform does not expose __version__ directly on its top-level import\n",
    "print(f\"google-cloud-pipeline-components version: {google_cloud_pipeline_components.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f4beba",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e51aacbe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import kfp\n",
    "import typing\n",
    "from typing import Dict\n",
    "from typing import NamedTuple\n",
    "from kfp import dsl\n",
    "from kfp.dsl import (Artifact,\n",
    "                        Dataset,\n",
    "                        Input,\n",
    "                        Model,\n",
    "                        Output,\n",
    "                        Metrics,\n",
    "                        ClassificationMetrics,\n",
    "                        component, \n",
    "                        OutputPath, \n",
    "                        InputPath)\n",
    "import google.cloud.aiplatform as aip\n",
    "from google_cloud_pipeline_components.types import artifact_types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed794de",
   "metadata": {},
   "source": [
    "Project and pipeline configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f5c603d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PROJECT_ID   = \"assignment1-476007\"        \n",
    "REGION       = \"us-central1\"\n",
    "PIPELINE_ROOT = \"gs://assignment1group3/runs\"\n",
    "DATA_BUCKET = \"assignment1group3\"\n",
    "DATA_FILE = 'data/penguins_clean.csv'\n",
    "MODEL_DIR = 'gs://assignment1group3/models'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fcfa33f",
   "metadata": {},
   "source": [
    "Pipeline component: Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "6b18e377",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    packages_to_install= [\"pandas\", \"google-cloud-storage\"],\n",
    "    base_image=\"python:3.10.7-slim\",\n",
    ")\n",
    "def download_data(project_id: str,\n",
    "                  bucket: str,\n",
    "                  file_name: str,\n",
    "                  dataset: Output[Dataset]):\n",
    "    \"\"\"Download data\"\"\"\n",
    "    from google.cloud import storage\n",
    "    import pandas as pd\n",
    "    import logging\n",
    "    import sys\n",
    "\n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "\n",
    "    client = storage.Client(project=project_id)\n",
    "    bucket = client.bucket(bucket)\n",
    "    blob = bucket.blob(file_name)\n",
    "    local_csv = dataset.path + '.csv'\n",
    "    blob.download_to_filename(local_csv)\n",
    "    logging.info(f\"Downloaded to {local_csv}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1005a4",
   "metadata": {},
   "source": [
    "Pipeline component: Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1c31f26d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "        packages_to_install=['pandas', 'scikit-learn'],\n",
    "        base_image=\"python:3.10.7-slim\",\n",
    "        )\n",
    "def preprocess(dataset: Input[Dataset],\n",
    "               train_out: Output[Dataset],\n",
    "               test_out: Output[Dataset]):\n",
    "    \n",
    "    import pandas as pd\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    import logging\n",
    "    import sys\n",
    "    import os\n",
    "\n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "\n",
    "    cols = [\"bill_length_mm\",\"bill_depth_mm\",\"flipper_length_mm\",\"body_mass_g\",\"species\"]\n",
    "    df = pd.read_csv(dataset.path + \".csv\")[cols].dropna()\n",
    "    train, test = train_test_split(df, test_size=0.2, random_state=42, stratify=df[\"species\"])\n",
    "    \n",
    "    os.makedirs(train_out.path, exist_ok=True)\n",
    "    train_csv_path = os.path.join(train_out.path, \"train.csv\")\n",
    "    train.to_csv(train_csv_path, index=False)\n",
    "    \n",
    "    os.makedirs(test_out.path, exist_ok=True)\n",
    "    test_csv_path = os.path.join(test_out.path, \"test.csv\")\n",
    "    test.to_csv(test_csv_path, index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc8fc63",
   "metadata": {},
   "source": [
    "Pipeline component: train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "dd789dbc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    base_image=\"python:3.10-slim\",\n",
    "    packages_to_install=[\"pandas\",\"scikit-learn\",\"joblib\"],\n",
    ")\n",
    "def train(\n",
    "    train_ds: Input[Dataset],   # expects train_ds.path/train.csv\n",
    "    model_art: Output[Model],   # directory to store model files\n",
    "):\n",
    "    \"\"\"Train sklearn pipeline, save model.pkl + model_meta.json into model_art.path\"\"\"\n",
    "    import os, sys, logging, json, joblib\n",
    "    import pandas as pd\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "\n",
    "    train_csv = os.path.join(train_ds.path, \"train.csv\")\n",
    "    logging.info(f\"[train] reading {train_csv}\")\n",
    "\n",
    "    df = pd.read_csv(train_csv)\n",
    "    X = df[[\"bill_length_mm\",\"bill_depth_mm\",\"flipper_length_mm\",\"body_mass_g\"]]\n",
    "    y = df[\"species\"]\n",
    "    \n",
    "    le = LabelEncoder()\n",
    "    y_encoded = le.fit_transform(y)\n",
    "\n",
    "    pipe = Pipeline(\n",
    "        steps=[\n",
    "            (\"scaler\", StandardScaler()),\n",
    "            (\"clf\", LogisticRegression(max_iter=500, multi_class=\"ovr\", random_state=42)),\n",
    "        ]\n",
    "    ).fit(X, y_encoded)\n",
    "\n",
    "    os.makedirs(model_art.path, exist_ok=True)\n",
    "    model_pkl = os.path.join(model_art.path, \"model.pkl\")\n",
    "    meta_json = os.path.join(model_art.path, \"model_meta.json\")\n",
    "\n",
    "    joblib.dump(pipe, model_pkl)\n",
    "    \n",
    "    with open(meta_json, \"w\") as f:\n",
    "        json.dump({\"classes\": sorted(y.unique())}, f)\n",
    "\n",
    "    logging.info(f\"[train] saved model to {model_pkl}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df17160a",
   "metadata": {},
   "source": [
    "Component: predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "b294d112",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    base_image=\"python:3.10-slim\",\n",
    "    packages_to_install=[\"pandas\",\"joblib\", \"scikit-learn\"],\n",
    ")\n",
    "def batch_predict(\n",
    "    model_art: Input[Model],      # expects model_art.path/model.pkl\n",
    "    features_ds: Input[Dataset],  # expects features_ds.path/test.csv (or features.csv)\n",
    "    predictions_out: Output[Dataset],  # writes predictions_out.path/preds.csv\n",
    "):\n",
    "    \"\"\"Run batch predictions on features and store preds.csv\"\"\"\n",
    "    import os, sys, logging, joblib\n",
    "    import pandas as pd\n",
    "\n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "\n",
    "    feat_csv  = os.path.join(features_ds.path, \"test.csv\")\n",
    "    model_pkl = os.path.join(model_art.path, \"model.pkl\")\n",
    "\n",
    "    logging.info(f\"[predict] reading features={feat_csv}\")\n",
    "    logging.info(f\"[predict] loading model={model_pkl}\")\n",
    "\n",
    "    X = pd.read_csv(feat_csv)[[\"bill_length_mm\",\"bill_depth_mm\",\"flipper_length_mm\",\"body_mass_g\"]]\n",
    "    model = joblib.load(model_pkl)\n",
    "    preds = model.predict(X)\n",
    "\n",
    "    os.makedirs(predictions_out.path, exist_ok=True)\n",
    "    out_csv = os.path.join(predictions_out.path, \"preds.csv\")\n",
    "    pd.DataFrame({\"prediction\": preds}).to_csv(out_csv, index=False)\n",
    "    logging.info(f\"[predict] wrote {out_csv}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e37778",
   "metadata": {},
   "source": [
    "Component: Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "51bf1b3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    base_image=\"python:3.10-slim\",\n",
    "    packages_to_install=[\"pandas\",\"joblib\",\"fsspec\",\"gcsfs\", 'scikit-learn'],\n",
    ")\n",
    "def evaluate_and_promote(\n",
    "    test_ds: Input[Dataset],   # expects test_ds.path/test.csv\n",
    "    model_art: Input[Model],   # expects model_art.path/model.pkl\n",
    "    metrics: Output[Metrics],\n",
    "    model_dir: str,            # e.g. gs://assignment1group3/models\n",
    "):\n",
    "    \"\"\"Compute accuracy and copy model files to a stable GCS location (promotion).\"\"\"\n",
    "    import os, sys, logging, json, joblib\n",
    "    import pandas as pd\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    import fsspec\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "\n",
    "    test_csv  = os.path.join(test_ds.path, \"test.csv\")\n",
    "    model_pkl = os.path.join(model_art.path, \"model.pkl\")\n",
    "    meta_json = os.path.join(model_art.path, \"model_meta.json\")\n",
    "\n",
    "    logging.info(f\"[eval] reading test={test_csv}\")\n",
    "    logging.info(f\"[eval] loading model={model_pkl}\")\n",
    "\n",
    "    df = pd.read_csv(test_csv)\n",
    "    X = df[[\"bill_length_mm\",\"bill_depth_mm\",\"flipper_length_mm\",\"body_mass_g\"]]\n",
    "    y = df[\"species\"]\n",
    "\n",
    "    model = joblib.load(model_pkl)\n",
    "    \n",
    "    with open(meta_json, 'r') as f:\n",
    "        meta = json.load(f)\n",
    "    classes = meta['classes']\n",
    "    \n",
    "    le = LabelEncoder()\n",
    "    le.fit(classes)\n",
    "    y_encoded = le.transform(y)\n",
    "    \n",
    "    predictions = model.predict(X)\n",
    "    acc = float(accuracy_score(y, predictions))\n",
    "    metrics.log_metric(\"accuracy\", acc)\n",
    "    logging.info(f\"[eval] accuracy={acc:.4f}\")\n",
    "\n",
    "    # Promote model files\n",
    "    proj_model_dir = model_dir.rstrip(\"/\")\n",
    "    fs = fsspec.filesystem(\"gcs\")\n",
    "    fs.put(model_pkl, f\"{proj_model_dir}/model.pkl\")\n",
    "    fs.put(meta_json, f\"{proj_model_dir}/model_meta.json\")\n",
    "    with fs.open(f\"{proj_model_dir}/metrics.json\", \"w\") as f:\n",
    "        json.dump({\"accuracy\": acc}, f)\n",
    "\n",
    "    logging.info(f\"[eval] promoted model to {proj_model_dir}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce6d085",
   "metadata": {},
   "source": [
    "Component: pipeline definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "d2e6271f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@kfp.dsl.pipeline(name=\"penguins-pipeline\")\n",
    "\n",
    "def penguins_pipeline(\n",
    "    project_id: str,\n",
    "    data_bucket: str,\n",
    "    data_file: str,\n",
    "    model_dir: str,\n",
    "    run_id: str = \"manual-run\",\n",
    "):\n",
    "    # 1) Ingest\n",
    "    ingest = download_data(\n",
    "        project_id=project_id,\n",
    "        bucket=data_bucket,\n",
    "        file_name=data_file,\n",
    "    )\n",
    "\n",
    "    # 2) Preprocess\n",
    "    prep = preprocess(\n",
    "        dataset=ingest.outputs[\"dataset\"]\n",
    "    )\n",
    "\n",
    "    # 3) Train\n",
    "    trn = train(\n",
    "        train_ds=prep.outputs[\"train_out\"]\n",
    "    )\n",
    "\n",
    "    # 4) Predict on test set (optional but nice to have)\n",
    "    pred = batch_predict(\n",
    "        model_art=trn.outputs[\"model_art\"],\n",
    "        features_ds=prep.outputs[\"test_out\"],\n",
    "    )\n",
    "\n",
    "    # 5) Evaluate + Promote\n",
    "    evaluate_and_promote(\n",
    "        test_ds=prep.outputs[\"test_out\"],\n",
    "        model_art=trn.outputs[\"model_art\"],\n",
    "        model_dir=model_dir,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d24864e",
   "metadata": {},
   "source": [
    "Compile into YAML file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "c8ae6eab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from kfp import compiler\n",
    "compiler.Compiler().compile(pipeline_func = penguins_pipeline, \n",
    "                            package_path = 'penguins_pipeline.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "448a3b7b-f957-4c76-b3ed-eba96375116b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import google.cloud.aiplatform as aip\n",
    "\n",
    "PROJECT_ID   = \"assignment1-476007\"\n",
    "REGION       = \"us-central1\"\n",
    "PIPELINE_ROOT = \"gs://assignment1group3/runs\"\n",
    "DATA_BUCKET  = \"assignment1group3\"\n",
    "DATA_FILE    = \"data/penguins_clean.csv\"\n",
    "MODEL_DIR    = \"gs://assignment1group3/models\"\n",
    "STAGING_BUCKET = \"gs://assignment1group3\"\n",
    "\n",
    "# Before initializing, make sure to set the GOOGLE_APPLICATION_CREDENTIALS\n",
    "# environment variable to the path of your service account.\n",
    "aip.init(\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    "    staging_bucket=STAGING_BUCKET,\n",
    ")\n",
    "\n",
    "# Prepare the pipeline job\n",
    "job = aip.PipelineJob(\n",
    "    display_name=\"penguins-pipeline-1\",\n",
    "    template_path=\"penguins_pipeline.yaml\",\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    parameter_values={\n",
    "        'project_id': PROJECT_ID,\n",
    "        'data_bucket': DATA_BUCKET,\n",
    "        'data_file': DATA_FILE,\n",
    "        'model_dir': MODEL_DIR,\n",
    "        'run_id' : 'run1' \n",
    "    }\n",
    ")\n",
    "\n",
    "job.run(enable_preflight_validations=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1809671-60df-451c-8414-8da20204251a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f24fd4c-ce9e-4309-8ce3-534133d46c77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c3ed2e-33c0-4f79-97ea-3857a82fc390",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73329615-6b0a-4ffa-b48c-203c65822434",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb966415-bdd6-40ba-9bff-331df9e0fdb6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m134",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m134"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
